{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ! pip install kaggle","metadata":{"id":"jxVqnstmc0xU","outputId":"320141db-5414-4478-fc67-14ad5eed8d76","execution":{"iopub.status.busy":"2022-01-08T21:32:07.708466Z","iopub.execute_input":"2022-01-08T21:32:07.708784Z","iopub.status.idle":"2022-01-08T21:32:07.729981Z","shell.execute_reply.started":"2022-01-08T21:32:07.7087Z","shell.execute_reply":"2022-01-08T21:32:07.729354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ! mkdir ~/.kaggle","metadata":{"id":"Aenc81LUc9Kn","execution":{"iopub.status.busy":"2022-01-08T21:32:07.731566Z","iopub.execute_input":"2022-01-08T21:32:07.731845Z","iopub.status.idle":"2022-01-08T21:32:07.735298Z","shell.execute_reply.started":"2022-01-08T21:32:07.731811Z","shell.execute_reply":"2022-01-08T21:32:07.734424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ! cp kaggle.json ~/.kaggle/\n# ! chmod 600 ~/.kaggle/kaggle.json","metadata":{"id":"Jmu-79VpdTv1","execution":{"iopub.status.busy":"2022-01-08T21:32:07.736609Z","iopub.execute_input":"2022-01-08T21:32:07.737055Z","iopub.status.idle":"2022-01-08T21:32:07.744581Z","shell.execute_reply.started":"2022-01-08T21:32:07.737015Z","shell.execute_reply":"2022-01-08T21:32:07.743928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ! kaggle datasets download hsankesara/flickr-image-dataset","metadata":{"id":"V7Q9ZVTAdixS","outputId":"5fe584ca-5108-4ec3-cf34-797bc2ba114a","execution":{"iopub.status.busy":"2022-01-08T21:32:07.746332Z","iopub.execute_input":"2022-01-08T21:32:07.746597Z","iopub.status.idle":"2022-01-08T21:32:07.75305Z","shell.execute_reply.started":"2022-01-08T21:32:07.746563Z","shell.execute_reply":"2022-01-08T21:32:07.752364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !unzip -qq swintransformer.zip\n# !unzip -qq flickr-image-dataset.zip","metadata":{"id":"AOk0Z9Dxdsa5","execution":{"iopub.status.busy":"2022-01-08T21:32:07.75483Z","iopub.execute_input":"2022-01-08T21:32:07.755078Z","iopub.status.idle":"2022-01-08T21:32:07.762291Z","shell.execute_reply.started":"2022-01-08T21:32:07.755045Z","shell.execute_reply":"2022-01-08T21:32:07.761603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport re\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport glob\nimport pandas as pd\nimport string\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n# from tensorflow.keras.applications import efficientnet\nfrom tensorflow.keras.layers import TextVectorization\n# import tensorflow_datasets as tfds\n\n# ds, ds_info = tfds.load('coco_captions', split='train', with_info=True)\n# fig = tfds.show_examples(ds, ds_info)\n\n\nseed = 111\nnp.random.seed(seed)\ntf.random.set_seed(seed)","metadata":{"id":"iFg1Rw3Qdv7V","execution":{"iopub.status.busy":"2022-01-12T12:19:36.113159Z","iopub.execute_input":"2022-01-12T12:19:36.113876Z","iopub.status.idle":"2022-01-12T12:19:36.129738Z","shell.execute_reply.started":"2022-01-12T12:19:36.113840Z","shell.execute_reply":"2022-01-12T12:19:36.128118Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.insert(1, '../input/swim-model')\nfrom model import SwinTransformer","metadata":{"execution":{"iopub.status.busy":"2022-01-12T12:19:36.132027Z","iopub.execute_input":"2022-01-12T12:19:36.132561Z","iopub.status.idle":"2022-01-12T12:19:36.140136Z","shell.execute_reply.started":"2022-01-12T12:19:36.132525Z","shell.execute_reply":"2022-01-12T12:19:36.139180Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# from swintransformer import SwinTransformer","metadata":{"id":"2mrmRS3Aeftc","execution":{"iopub.status.busy":"2022-01-11T11:19:50.40449Z","iopub.execute_input":"2022-01-11T11:19:50.404975Z","iopub.status.idle":"2022-01-11T11:19:50.408659Z","shell.execute_reply.started":"2022-01-11T11:19:50.404941Z","shell.execute_reply":"2022-01-11T11:19:50.408049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Path to the images\nIMAGES_PATH = \"../input/flickr-image-dataset/flickr30k_images/flickr30k_images\"\n\n# # Desired image dimensions\nIMAGE_SIZE = (224, 224)\n\n# Vocabulary size\nVOCAB_SIZE = 9000\n\n# Fixed length allowed for any sequence\nSEQ_LENGTH = 20\n\n# Dimension for the image embeddings and token embeddings\nEMBED_DIM = 512\n\n# Per-layer units in the feed-forward network\nFF_DIM = 512\n\n# Other training parameters\nBATCH_SIZE = 64\nEPOCHS = 52\nAUTOTUNE = tf.data.AUTOTUNE","metadata":{"id":"dBnTbCMiehfM","execution":{"iopub.status.busy":"2022-01-12T12:19:36.986892Z","iopub.execute_input":"2022-01-12T12:19:36.987155Z","iopub.status.idle":"2022-01-12T12:19:36.992379Z","shell.execute_reply.started":"2022-01-12T12:19:36.987126Z","shell.execute_reply":"2022-01-12T12:19:36.991633Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"LOCAL_FLICKR_PATH = '../input/flickr-image-dataset/flickr30k_images/'\nannotation_file = LOCAL_FLICKR_PATH + 'results.csv'\nLOCAL_IMG_PATH = LOCAL_FLICKR_PATH + 'flickr30k_images/'\n\n!ls {LOCAL_IMG_PATH} | wc","metadata":{"id":"u1LFtZYpeq7e","outputId":"d9f41240-ff96-4471-de29-9c51f3492f32","execution":{"iopub.status.busy":"2022-01-12T12:19:44.461362Z","iopub.execute_input":"2022-01-12T12:19:44.462056Z","iopub.status.idle":"2022-01-12T12:19:45.623956Z","shell.execute_reply.started":"2022-01-12T12:19:44.462019Z","shell.execute_reply":"2022-01-12T12:19:45.623162Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#Image Path\n# Image path given ------ used glob to get list of all images\nimage_path = \"../input/flickr-image-dataset/flickr30k_images/flickr30k_images/\"\njpgs = os.listdir(image_path)\nimages = glob.glob(image_path+'*.jpg')\n\n\n\n#Caption Path\n# Read the captions with Pandas ---- changed coloumn names\ncaption_path = '../input/flickr-image-dataset/flickr30k_images/results.csv'\ncaptions = pd.read_csv(caption_path, sep = '|')\ndf = pd.DataFrame(captions)\ndf = df.rename(columns={'image_name' : 'filename',' comment_number': 'index', ' comment': 'caption'}, inplace = False)\ndf = df.reindex(columns =['index','filename','caption'])\ndata = df[df['caption'].notnull()]\n\n\n\n#Printing DataFrame\n# DataFrame name is Data\nprint(\"Total Images in Dataset = {}\".format(len(jpgs)))\ndata.head()","metadata":{"id":"sApfIQoees_P","outputId":"2d1a12e1-6b9d-4496-f4ff-da17b357a002","execution":{"iopub.status.busy":"2022-01-12T12:19:48.485849Z","iopub.execute_input":"2022-01-12T12:19:48.486165Z","iopub.status.idle":"2022-01-12T12:19:49.116841Z","shell.execute_reply.started":"2022-01-12T12:19:48.486130Z","shell.execute_reply":"2022-01-12T12:19:49.116121Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Made a list of images, above list had sets of images, this also has replciated images \n# ( 5 same image paths of 5 captions)\nall_img_name_vector = []\nfor annot in data[\"filename\"]:\n   full_image_path = image_path + annot\n   all_img_name_vector.append(full_image_path)\n\n#To know lenght of list\nlen(all_img_name_vector)","metadata":{"id":"UASEMRUveyAB","outputId":"cb343543-3596-4fec-a4a7-0ceb3aa368dd","execution":{"iopub.status.busy":"2022-01-12T12:19:52.063181Z","iopub.execute_input":"2022-01-12T12:19:52.063632Z","iopub.status.idle":"2022-01-12T12:19:52.137834Z","shell.execute_reply.started":"2022-01-12T12:19:52.063592Z","shell.execute_reply":"2022-01-12T12:19:52.137163Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"plt.imshow(plt.imread('../input/flickr-image-dataset/flickr30k_images/flickr30k_images/1000092795.jpg'))","metadata":{"id":"1mLkMHDsvJ7U","outputId":"fa7b2471-21fa-4afa-9f8d-f42e26e484fd","execution":{"iopub.status.busy":"2022-01-12T12:19:54.601381Z","iopub.execute_input":"2022-01-12T12:19:54.601808Z","iopub.status.idle":"2022-01-12T12:19:54.862332Z","shell.execute_reply.started":"2022-01-12T12:19:54.601776Z","shell.execute_reply":"2022-01-12T12:19:54.861647Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"data_with_seq = data.copy()\ndata_with_seq.caption = data_with_seq.caption.apply(lambda x: \"<start> \" + x.strip() + \" <end>\")","metadata":{"id":"J2aJe__vwzZS","execution":{"iopub.status.busy":"2022-01-12T12:19:59.124754Z","iopub.execute_input":"2022-01-12T12:19:59.125031Z","iopub.status.idle":"2022-01-12T12:19:59.226208Z","shell.execute_reply.started":"2022-01-12T12:19:59.125001Z","shell.execute_reply":"2022-01-12T12:19:59.225363Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_dict = {}\nfor i in range(len(all_img_name_vector[:-10000])):\n  if train_dict.get(all_img_name_vector[i],None) == None:\n    train_dict[all_img_name_vector[i]] = [data.caption.iloc[i]]\n  else:\n    train_dict[all_img_name_vector[i]].append(data.caption.iloc[i])","metadata":{"id":"SGTDnustus4w","execution":{"iopub.status.busy":"2022-01-12T12:20:00.896219Z","iopub.execute_input":"2022-01-12T12:20:00.896502Z","iopub.status.idle":"2022-01-12T12:20:03.052676Z","shell.execute_reply.started":"2022-01-12T12:20:00.896453Z","shell.execute_reply":"2022-01-12T12:20:03.051941Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train_data = train_dict.copy()\nfor k, v in train_dict.items():\n  if len(v) < 5:\n    train_data.pop(k)","metadata":{"id":"rARAndO9OGby","execution":{"iopub.status.busy":"2022-01-12T12:20:03.824045Z","iopub.execute_input":"2022-01-12T12:20:03.824646Z","iopub.status.idle":"2022-01-12T12:20:03.839815Z","shell.execute_reply.started":"2022-01-12T12:20:03.824594Z","shell.execute_reply":"2022-01-12T12:20:03.837650Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"test_dict = {}\nfor i in range(len(all_img_name_vector[-10000:])):\n  if test_dict.get(all_img_name_vector[i],None) == None:\n    test_dict[all_img_name_vector[i]] = [data.caption.iloc[i]]\n  else:\n    test_dict[all_img_name_vector[i]].append(data.caption.iloc[i])","metadata":{"id":"txwatfNixxDT","execution":{"iopub.status.busy":"2022-01-12T12:20:06.097518Z","iopub.execute_input":"2022-01-12T12:20:06.098147Z","iopub.status.idle":"2022-01-12T12:20:06.245992Z","shell.execute_reply.started":"2022-01-12T12:20:06.098110Z","shell.execute_reply":"2022-01-12T12:20:06.245314Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"test_data = test_dict.copy()\nfor k, v in test_dict.items():\n  if len(v) < 5:\n    test_data.pop(k)","metadata":{"id":"VLlcN3e_Ot2q","execution":{"iopub.status.busy":"2022-01-12T12:20:08.825293Z","iopub.execute_input":"2022-01-12T12:20:08.825585Z","iopub.status.idle":"2022-01-12T12:20:08.830214Z","shell.execute_reply.started":"2022-01-12T12:20:08.825552Z","shell.execute_reply":"2022-01-12T12:20:08.829523Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Vectorizing the text data\n\nWe'll use the `TextVectorization` layer to vectorize the text data,\nthat is to say, to turn the\noriginal strings into integer sequences where each integer represents the index of\na word in a vocabulary. We will use a custom string standardization scheme\n(strip punctuation characters except `<` and `>`) and the default\nsplitting scheme (split on whitespace).","metadata":{"id":"wX2Reiz0Pb7i"}},{"cell_type":"code","source":"def custom_standardization(input_string):\n    lowercase = tf.strings.lower(input_string)\n    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n\n\nstrip_chars = \"!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\nstrip_chars = strip_chars.replace(\"<\", \"\")\nstrip_chars = strip_chars.replace(\">\", \"\")\n\nvectorization = TextVectorization(\n    max_tokens=VOCAB_SIZE,\n    output_mode=\"int\",\n    output_sequence_length=SEQ_LENGTH,\n    standardize=custom_standardization,\n)\nvectorization.adapt(data.caption.values)\n\n# Data augmentation for image data\nimage_augmentation = keras.Sequential(\n    [\n        layers.RandomFlip(\"horizontal\"),\n        layers.RandomRotation(0.2),\n        layers.RandomContrast(0.3),\n    ]\n)\n","metadata":{"id":"Q5o1dzakpnws","execution":{"iopub.status.busy":"2022-01-12T12:20:11.526141Z","iopub.execute_input":"2022-01-12T12:20:11.526420Z","iopub.status.idle":"2022-01-12T12:20:23.507091Z","shell.execute_reply.started":"2022-01-12T12:20:11.526389Z","shell.execute_reply":"2022-01-12T12:20:23.506403Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Building a `tf.data.Dataset` pipeline for training\n\nWe will generate pairs of images and corresponding captions using a `tf.data.Dataset` object.\nThe pipeline consists of two steps:\n\n1. Read the image from the disk\n2. Tokenize all the five captions corresponding to the image","metadata":{"id":"7y9-ITI5PXMj"}},{"cell_type":"code","source":"\ndef decode_and_resize(img_path):\n    img = tf.io.read_file(img_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.resize(img, IMAGE_SIZE)\n    img = tf.image.convert_image_dtype(img, tf.float32)\n    return img\n\n\ndef process_input(img_path, captions):\n    return decode_and_resize(img_path), vectorization(captions)\n\n\ndef make_dataset(images, captions):\n    dataset = tf.data.Dataset.from_tensor_slices((images, captions))\n    dataset = dataset.shuffle(len(images))\n    dataset = dataset.map(process_input, num_parallel_calls=AUTOTUNE)\n    dataset = dataset.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n\n    return dataset\n\n\n# Pass the list of images and the list of corresponding captions\ntrain_dataset = make_dataset(list(train_data.keys()), list(train_data.values()))\n\nvalid_dataset = make_dataset(list(test_data.keys()), list(test_data.values()))\n","metadata":{"id":"p3HYUi0Uyww3","execution":{"iopub.status.busy":"2022-01-12T12:20:23.508774Z","iopub.execute_input":"2022-01-12T12:20:23.509019Z","iopub.status.idle":"2022-01-12T12:20:24.453926Z","shell.execute_reply.started":"2022-01-12T12:20:23.508986Z","shell.execute_reply":"2022-01-12T12:20:24.453181Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## Building the model\n\nOur image captioning architecture consists of three models:\n\n1. A CNN: used to extract the image features\n2. A TransformerEncoder: The extracted image features are then passed to a Transformer\n                    based encoder that generates a new representation of the inputs\n3. A TransformerDecoder: This model takes the encoder output and the text data\n                    (sequences) as inputs and tries to learn to generate the caption.","metadata":{"id":"KhstPPoWPTD5"}},{"cell_type":"code","source":"\ndef get_cnn_model():\n    base_model = keras.models.Sequential([\n                                          keras.layers.Lambda(\n                                              lambda data: keras.applications.imagenet_utils.preprocess_input(\n                                                  tf.cast(data, tf.float32), mode=\"torch\"),\n                                                   input_shape=[*IMAGE_SIZE, 3]),\n                                          SwinTransformer('swin_base_224', include_top=False, pretrained=True)])\n    # efficientnet.EfficientNetB3(\n    #     input_shape=(*IMAGE_SIZE, 3), include_top=False, weights=\"imagenet\",\n    # )\n    # We freeze our feature extractor\n    base_model.trainable = False\n    base_model_out = base_model.output\n    base_model_out = layers.Reshape((-1, base_model_out.shape[-1]))(base_model_out)\n    cnn_model = keras.models.Model(base_model.input, base_model_out)\n    return cnn_model\n\n\nclass TransformerEncoderBlock(layers.Layer):\n    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.dense_dim = dense_dim\n        self.num_heads = num_heads\n        self.attention_1 = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim, dropout=0.0\n        )\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n        self.dense_1 = layers.Dense(embed_dim, activation=\"relu\")\n\n    def call(self, inputs, training, mask=None):\n        inputs = self.layernorm_1(inputs)\n        inputs = self.dense_1(inputs)\n\n        attention_output_1 = self.attention_1(\n            query=inputs,\n            value=inputs,\n            key=inputs,\n            attention_mask=None,\n            training=training,\n        )\n        out_1 = self.layernorm_2(inputs + attention_output_1)\n        return out_1\n\n\nclass PositionalEmbedding(layers.Layer):\n    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n        super().__init__(**kwargs)\n        self.token_embeddings = layers.Embedding(\n            input_dim=vocab_size, output_dim=embed_dim\n        )\n        self.position_embeddings = layers.Embedding(\n            input_dim=sequence_length, output_dim=embed_dim\n        )\n        self.sequence_length = sequence_length\n        self.vocab_size = vocab_size\n        self.embed_dim = embed_dim\n        self.embed_scale = tf.math.sqrt(tf.cast(embed_dim, tf.float32))\n\n    def call(self, inputs):\n        length = tf.shape(inputs)[-1]\n        positions = tf.range(start=0, limit=length, delta=1)\n        embedded_tokens = self.token_embeddings(inputs)\n        embedded_tokens = embedded_tokens * self.embed_scale\n        embedded_positions = self.position_embeddings(positions)\n        return embedded_tokens + embedded_positions\n\n    def compute_mask(self, inputs, mask=None):\n        return tf.math.not_equal(inputs, 0)\n\n\nclass TransformerDecoderBlock(layers.Layer):\n    def __init__(self, embed_dim, ff_dim, num_heads, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.ff_dim = ff_dim\n        self.num_heads = num_heads\n        self.attention_1 = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n        )\n        self.attention_2 = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n        )\n        self.ffn_layer_1 = layers.Dense(ff_dim, activation=\"relu\")\n        self.ffn_layer_2 = layers.Dense(embed_dim)\n\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n        self.layernorm_3 = layers.LayerNormalization()\n\n        self.embedding = PositionalEmbedding(\n            embed_dim=EMBED_DIM, sequence_length=SEQ_LENGTH, vocab_size=VOCAB_SIZE\n        )\n        self.out = layers.Dense(VOCAB_SIZE, activation=\"softmax\")\n\n        self.dropout_1 = layers.Dropout(0.3)\n        self.dropout_2 = layers.Dropout(0.5)\n        self.supports_masking = True\n\n    def call(self, inputs, encoder_outputs, training, mask=None):\n        inputs = self.embedding(inputs)\n        causal_mask = self.get_causal_attention_mask(inputs)\n\n        if mask is not None:\n            padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)\n            combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)\n            combined_mask = tf.minimum(combined_mask, causal_mask)\n\n        attention_output_1 = self.attention_1(\n            query=inputs,\n            value=inputs,\n            key=inputs,\n            attention_mask=combined_mask,\n            training=training,\n        )\n        out_1 = self.layernorm_1(inputs + attention_output_1)\n\n        attention_output_2 = self.attention_2(\n            query=out_1,\n            value=encoder_outputs,\n            key=encoder_outputs,\n            attention_mask=padding_mask,\n            training=training,\n        )\n        out_2 = self.layernorm_2(out_1 + attention_output_2)\n\n        ffn_out = self.ffn_layer_1(out_2)\n        ffn_out = self.dropout_1(ffn_out, training=training)\n        ffn_out = self.ffn_layer_2(ffn_out)\n\n        ffn_out = self.layernorm_3(ffn_out + out_2, training=training)\n        ffn_out = self.dropout_2(ffn_out, training=training)\n        preds = self.out(ffn_out)\n        return preds\n\n    def get_causal_attention_mask(self, inputs):\n        input_shape = tf.shape(inputs)\n        batch_size, sequence_length = input_shape[0], input_shape[1]\n        i = tf.range(sequence_length)[:, tf.newaxis]\n        j = tf.range(sequence_length)\n        mask = tf.cast(i >= j, dtype=\"int32\")\n        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n        mult = tf.concat(\n            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n            axis=0,\n        )\n        return tf.tile(mask, mult)\n\n\nclass ImageCaptioningModel(keras.Model):\n    def __init__(\n        self, cnn_model, encoder, decoder, num_captions_per_image=5, image_aug=None,\n    ):\n        super().__init__()\n        self.cnn_model = cnn_model\n        self.encoder = encoder\n        self.decoder = decoder\n        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n        self.acc_tracker = keras.metrics.Mean(name=\"accuracy\")\n        self.num_captions_per_image = num_captions_per_image\n        self.image_aug = image_aug\n\n    def calculate_loss(self, y_true, y_pred, mask):\n        loss = self.loss(y_true, y_pred)\n        mask = tf.cast(mask, dtype=loss.dtype)\n        loss *= mask\n        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n\n    def calculate_accuracy(self, y_true, y_pred, mask):\n        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))\n        accuracy = tf.math.logical_and(mask, accuracy)\n        accuracy = tf.cast(accuracy, dtype=tf.float32)\n        mask = tf.cast(mask, dtype=tf.float32)\n        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)\n\n    def _compute_caption_loss_and_acc(self, img_embed, batch_seq, training=True):\n        encoder_out = self.encoder(img_embed, training=training)\n        batch_seq_inp = batch_seq[:, :-1]\n        batch_seq_true = batch_seq[:, 1:]\n        mask = tf.math.not_equal(batch_seq_true, 0)\n        batch_seq_pred = self.decoder(\n            batch_seq_inp, encoder_out, training=training, mask=mask\n        )\n        loss = self.calculate_loss(batch_seq_true, batch_seq_pred, mask)\n        acc = self.calculate_accuracy(batch_seq_true, batch_seq_pred, mask)\n        return loss, acc\n\n    def train_step(self, batch_data):\n        batch_img, batch_seq = batch_data\n        batch_loss = 0\n        batch_acc = 0\n\n        if self.image_aug:\n            batch_img = self.image_aug(batch_img)\n\n        # 1. Get image embeddings\n        img_embed = self.cnn_model(batch_img)\n\n        # 2. Pass each of the five captions one by one to the decoder\n        # along with the encoder outputs and compute the loss as well as accuracy\n        # for each caption.\n        for i in range(self.num_captions_per_image):\n            with tf.GradientTape() as tape:\n                loss, acc = self._compute_caption_loss_and_acc(\n                    img_embed, batch_seq[:, i, :], training=True\n                )\n\n                # 3. Update loss and accuracy\n                batch_loss += loss\n                batch_acc += acc\n\n            # 4. Get the list of all the trainable weights\n            train_vars = (\n                self.encoder.trainable_variables + self.decoder.trainable_variables\n            )\n\n            # 5. Get the gradients\n            grads = tape.gradient(loss, train_vars)\n\n            # 6. Update the trainable weights\n            self.optimizer.apply_gradients(zip(grads, train_vars))\n\n        # 7. Update the trackers\n        batch_acc /= float(self.num_captions_per_image)\n        self.loss_tracker.update_state(batch_loss)\n        self.acc_tracker.update_state(batch_acc)\n\n        # 8. Return the loss and accuracy values\n        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n\n    def test_step(self, batch_data):\n        batch_img, batch_seq = batch_data\n        batch_loss = 0\n        batch_acc = 0\n\n        # 1. Get image embeddings\n        img_embed = self.cnn_model(batch_img)\n\n        # 2. Pass each of the five captions one by one to the decoder\n        # along with the encoder outputs and compute the loss as well as accuracy\n        # for each caption.\n        for i in range(self.num_captions_per_image):\n            loss, acc = self._compute_caption_loss_and_acc(\n                img_embed, batch_seq[:, i, :], training=False\n            )\n\n            # 3. Update batch loss and batch accuracy\n            batch_loss += loss\n            batch_acc += acc\n\n        batch_acc /= float(self.num_captions_per_image)\n\n        # 4. Update the trackers\n        self.loss_tracker.update_state(batch_loss)\n        self.acc_tracker.update_state(batch_acc)\n\n        # 5. Return the loss and accuracy values\n        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n\n    @property\n    def metrics(self):\n        # We need to list our metrics here so the `reset_states()` can be\n        # called automatically.\n        return [self.loss_tracker, self.acc_tracker]\n\n\ncnn_model = get_cnn_model()\nencoder = TransformerEncoderBlock(embed_dim=EMBED_DIM, dense_dim=FF_DIM, num_heads=1)\ndecoder = TransformerDecoderBlock(embed_dim=EMBED_DIM, ff_dim=FF_DIM, num_heads=2)\ncaption_model = ImageCaptioningModel(\n    cnn_model=cnn_model, encoder=encoder, decoder=decoder, image_aug=image_augmentation,\n)","metadata":{"id":"t4PCtSssCD79","outputId":"066f5382-4ad5-478b-8e5d-4e51baf5c7fa","execution":{"iopub.status.busy":"2022-01-12T12:21:53.590955Z","iopub.execute_input":"2022-01-12T12:21:53.591247Z","iopub.status.idle":"2022-01-12T12:22:11.571256Z","shell.execute_reply.started":"2022-01-12T12:21:53.591213Z","shell.execute_reply":"2022-01-12T12:22:11.570534Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# Model Training","metadata":{"id":"t8nQTIp6PK40"}},{"cell_type":"code","source":"# Define the loss function\ncross_entropy = keras.losses.SparseCategoricalCrossentropy(\n    from_logits=False, reduction=\"none\"\n)\n\n# EarlyStopping criteria\n# early_stopping = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n\n\n# Learning Rate Scheduler for the optimizer\nclass LRSchedule(keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, post_warmup_learning_rate, warmup_steps):\n        super().__init__()\n        self.post_warmup_learning_rate = post_warmup_learning_rate\n        self.warmup_steps = warmup_steps\n\n    def __call__(self, step):\n        global_step = tf.cast(step, tf.float32)\n        warmup_steps = tf.cast(self.warmup_steps, tf.float32)\n        warmup_progress = global_step / warmup_steps\n        warmup_learning_rate = self.post_warmup_learning_rate * warmup_progress\n        return tf.cond(\n            global_step < warmup_steps,\n            lambda: warmup_learning_rate,\n            lambda: self.post_warmup_learning_rate,\n        )\n\n\n# Create a learning rate schedule\nnum_train_steps = len(train_dataset) * EPOCHS\nnum_warmup_steps = num_train_steps // 15\nlr_schedule = LRSchedule(post_warmup_learning_rate=1e-4, warmup_steps=num_warmup_steps)\n\n# Compile the model\ncaption_model.compile(optimizer=keras.optimizers.Adam(lr_schedule), loss=cross_entropy)\nmy_callbacks = [\n    tf.keras.callbacks.EarlyStopping(patience=2,restore_best_weights=True),\n    \n    tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n]\n# Fit the model\ncaption_model.fit(\n    train_dataset,\n    epochs=EPOCHS,\n    validation_data=valid_dataset,\n    callbacks=my_callbacks,\n)","metadata":{"id":"TE_t6W3-PF3L","outputId":"9ccd798f-a7e5-4029-9eb1-5b50067cdd4e","execution":{"iopub.status.busy":"2022-01-12T12:54:55.632658Z","iopub.execute_input":"2022-01-12T12:54:55.632935Z","iopub.status.idle":"2022-01-12T19:16:30.711074Z","shell.execute_reply.started":"2022-01-12T12:54:55.632905Z","shell.execute_reply":"2022-01-12T19:16:30.710302Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"train_loss=[26.9275,18.8002,17.2580,16.4341,15.8876,15.4700,15.1338,14.8521,14.6026,14.3928,14.1907,14.0111,13.7009,13.5562,13.4302,13.3108,13.0832,12.9825,12.8888,12.8033,12.7566,12.6366]\nval_loss=[19.1178,16.4775,15.2753,14.5390,13.9776,13.5779,13.2489,12.9007,12.6872,12.4396,12.2091,11.9688,11.8315,11.6511,11.4848,11.3461,11.2180,11.0490,10.9646,10.7944,10.7315,10.6324]\nvocab=\"10000\"\nepoch='20'\ntime_of_epoch=\"8 min\"\nemb=\"512\"\nmodel=\"swin transformer\"","metadata":{"execution":{"iopub.status.busy":"2022-01-12T19:19:14.746647Z","iopub.execute_input":"2022-01-12T19:19:14.746909Z","iopub.status.idle":"2022-01-12T19:19:14.753686Z","shell.execute_reply.started":"2022-01-12T19:19:14.746879Z","shell.execute_reply":"2022-01-12T19:19:14.752864Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nx=[i for i in range(len(val_loss))]\nplt.figure(figsize=(6,6))\nplt.plot(x,train_loss,label=\"train_loss\")\nplt.plot(x,val_loss,label=\"val_loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.title(\"model=swin transformer     vocab=10000        batch_size = 64      n_epochs = 22     time_of_epoch=8 min    emb=512 \")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-12T19:19:17.550965Z","iopub.execute_input":"2022-01-12T19:19:17.551240Z","iopub.status.idle":"2022-01-12T19:19:17.784001Z","shell.execute_reply.started":"2022-01-12T19:19:17.551206Z","shell.execute_reply":"2022-01-12T19:19:17.783336Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"train_loss=[17.7250,16.9968,16.3065,15.7969,15.4110,15.0838,14.8068,14.5705,14.3611,14.1762,13.9982,13.8393,13.6898,13.5541,13.4355,13.3116,13.1957,13.0879,12.9939,12.8931,12.8036,12.7163,12.6301,12.5436,12.4753,12.4060,12.3328,12.2716,12.2022,12.1536,12.0788,12.0348,12.0348,11.9667,11.9150,11.8722,11.8157,11.7658,11.7224,11.6730,11.6257,11.5912,11.5414,11.5101,11.4517,11.4205,11.3940,11.3683,11.3214,11.2865,11.2571,11.2252]\nval_loss  =[15.9586,15.0190,14.3488,13.8624,13.5438,13.1446,12.8397,12.5733,12.3711,12.1182,11.9441,11.7475,11.6333,11.4661,11.3414,11.2005,11.0793,10.9239,10.7995,10.7441,10.6363,10.5754,10.4139,10.3335,10.2568,10.1475,10.1002,10.0447,9.8908,9.8353,9.7905,9.7112,9.7112,9.7182,9.6517,9.5367,9.5034,9.4167,9.4034,9.3124,9.2971,9.2589,9.1639,9.1320,9.0487,9.0450,9.0282,8.9623,8.9436,8.9047,8.8269,8.8697]","metadata":{"execution":{"iopub.status.busy":"2022-01-12T19:24:37.108894Z","iopub.execute_input":"2022-01-12T19:24:37.109164Z","iopub.status.idle":"2022-01-12T19:24:37.117909Z","shell.execute_reply.started":"2022-01-12T19:24:37.109128Z","shell.execute_reply":"2022-01-12T19:24:37.116842Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nx=[i for i in range(len(val_loss))]\nplt.figure(figsize=(6,6))\nplt.plot(x,train_loss,label=\"train_loss\")\nplt.plot(x,val_loss,label=\"val_loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.title(\"model=swin transformer     vocab=9000        batch_size = 64      n_epochs = 52     time_of_epoch=8 min    emb=512 \")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-12T19:24:39.750084Z","iopub.execute_input":"2022-01-12T19:24:39.750342Z","iopub.status.idle":"2022-01-12T19:24:39.953887Z","shell.execute_reply.started":"2022-01-12T19:24:39.750304Z","shell.execute_reply":"2022-01-12T19:24:39.953113Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"caption_model.save_weights('weights.h5')","metadata":{"id":"M9a6R5NbPeGb","execution":{"iopub.status.busy":"2022-01-12T19:17:00.087417Z","iopub.execute_input":"2022-01-12T19:17:00.088377Z","iopub.status.idle":"2022-01-12T19:17:00.921445Z","shell.execute_reply.started":"2022-01-12T19:17:00.088336Z","shell.execute_reply":"2022-01-12T19:17:00.920727Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-01-12T12:44:40.360275Z","iopub.execute_input":"2022-01-12T12:44:40.360920Z","iopub.status.idle":"2022-01-12T12:44:41.085298Z","shell.execute_reply.started":"2022-01-12T12:44:40.360874Z","shell.execute_reply":"2022-01-12T12:44:41.084313Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"caption_model=keras.models.Sequential()\ncaption_model.load_model(\"./weights.h5\")\n","metadata":{"execution":{"iopub.status.busy":"2022-01-11T20:04:42.685458Z","iopub.execute_input":"2022-01-11T20:04:42.685777Z","iopub.status.idle":"2022-01-11T20:04:42.711603Z","shell.execute_reply.started":"2022-01-11T20:04:42.685746Z","shell.execute_reply":"2022-01-11T20:04:42.710313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# caption_model.load_weights(\"./swin_weights_30k_13.h5\")\nreconstructed_model = keras.models.load_model(\"./weights.h5\")","metadata":{"execution":{"iopub.status.busy":"2022-01-11T20:05:35.006939Z","iopub.execute_input":"2022-01-11T20:05:35.007372Z","iopub.status.idle":"2022-01-11T20:05:35.037129Z","shell.execute_reply.started":"2022-01-11T20:05:35.007308Z","shell.execute_reply":"2022-01-11T20:05:35.035313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cat weights.h5","metadata":{"execution":{"iopub.status.busy":"2022-01-11T20:05:57.593032Z","iopub.execute_input":"2022-01-11T20:05:57.593356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath=\"../input/swim-model/weights.h5\", verbose=1)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-11T19:56:18.06627Z","iopub.execute_input":"2022-01-11T19:56:18.066951Z","iopub.status.idle":"2022-01-11T19:56:18.071602Z","shell.execute_reply.started":"2022-01-11T19:56:18.066915Z","shell.execute_reply":"2022-01-11T19:56:18.070572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit the model\ncaption_model.fit(\n    train_dataset,\n    epochs=EPOCHS,\n    validation_data=valid_dataset,\n    callbacks=[early_stopping],\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-11T17:58:11.990923Z","iopub.status.idle":"2022-01-11T17:58:11.991515Z","shell.execute_reply.started":"2022-01-11T17:58:11.991269Z","shell.execute_reply":"2022-01-11T17:58:11.991295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab = vectorization.get_vocabulary()\nindex_lookup = dict(zip(range(len(vocab)), vocab))\nmax_decoded_sentence_length = SEQ_LENGTH - 1\nvalid_images = list(test_data.keys())\n\n\ndef generate_caption():\n    # Select a random image from the validation dataset\n    sample_img = np.random.choice(valid_images)\n\n    # Read the image from the disk\n    sample_img = decode_and_resize(sample_img)\n    img = sample_img.numpy().clip(0, 255).astype(np.uint8)\n    plt.imshow(img)\n    plt.show()\n\n    # Pass the image to the CNN\n    img = tf.expand_dims(sample_img, 0)\n    img = caption_model.cnn_model(img)\n\n    # Pass the image features to the Transformer encoder\n    encoded_img = caption_model.encoder(img, training=False)\n\n    # Generate the caption using the Transformer decoder\n    decoded_caption = \"<start> \"\n    for i in range(max_decoded_sentence_length):\n        tokenized_caption = vectorization([decoded_caption])[:, :-1]\n        mask = tf.math.not_equal(tokenized_caption, 0)\n        predictions = caption_model.decoder(\n            tokenized_caption, encoded_img, training=False, mask=mask\n        )\n        sampled_token_index = np.argmax(predictions[0, i, :])\n        sampled_token = index_lookup[sampled_token_index]\n        if sampled_token == \" <end>\":\n            break\n        decoded_caption += \" \" + sampled_token\n\n    decoded_caption = decoded_caption.replace(\"<start> \", \"\")\n    decoded_caption = decoded_caption.replace(\" <end>\", \"\").strip()\n    print(\"Predicted Caption: \", decoded_caption)\n\n\n# Check predictions for a few samples\ngenerate_caption()\ngenerate_caption()\ngenerate_caption()\ngenerate_caption()\ngenerate_caption()\ngenerate_caption()\ngenerate_caption()\ngenerate_caption()\ngenerate_caption()\ngenerate_caption()\ngenerate_caption()\ngenerate_caption()","metadata":{"execution":{"iopub.status.busy":"2022-01-12T19:31:48.070699Z","iopub.execute_input":"2022-01-12T19:31:48.070958Z","iopub.status.idle":"2022-01-12T19:31:59.087686Z","shell.execute_reply.started":"2022-01-12T19:31:48.070929Z","shell.execute_reply":"2022-01-12T19:31:59.086956Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"vocab = vectorization.get_vocabulary()\nindex_lookup = dict(zip(range(len(vocab)), vocab))\nmax_decoded_sentence_length = SEQ_LENGTH - 1\nvalid_images = list(test_data.keys())\n\n\ndef generate_caption(test_img):\n    # Select a random image from the validation dataset\n    # sample_img = np.random.choice(valid_images)\n\n    # Read the image from the disk\n    sample_img = decode_and_resize(test_img)\n    # img = sample_img.numpy().clip(0, 255).astype(np.uint8)\n    # plt.imshow(img)\n    # plt.show()\n\n    # Pass the image to the CNN\n    img = tf.expand_dims(sample_img, 0)\n    img = caption_model.cnn_model(img)\n\n    # Pass the image features to the Transformer encoder\n    encoded_img = caption_model.encoder(img, training=False)\n\n    # Generate the caption using the Transformer decoder\n    decoded_caption = \"<start> \"\n    for i in range(max_decoded_sentence_length):\n        tokenized_caption = vectorization([decoded_caption])[:, :-1]\n        mask = tf.math.not_equal(tokenized_caption, 0)\n        predictions = caption_model.decoder(\n            tokenized_caption, encoded_img, training=False, mask=mask\n        )\n        sampled_token_index = np.argmax(predictions[0, i, :])\n        sampled_token = index_lookup[sampled_token_index]\n        if sampled_token == \" <end>\":\n            break\n        decoded_caption += \" \" + sampled_token\n\n    decoded_caption = decoded_caption.replace(\"<start> \", \"\")\n    decoded_caption = decoded_caption.replace(\" <end>\", \"\").strip()\n    # print(\"Predicted Caption: \", decoded_caption)\n    return decoded_caption\n\n\n# Check predictions for a few samples\n# generate_caption()\n# generate_caption()\n# generate_caption()","metadata":{"execution":{"iopub.status.busy":"2022-01-12T19:33:58.627005Z","iopub.execute_input":"2022-01-12T19:33:58.627289Z","iopub.status.idle":"2022-01-12T19:33:58.666122Z","shell.execute_reply.started":"2022-01-12T19:33:58.627258Z","shell.execute_reply":"2022-01-12T19:33:58.665386Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"from nltk.translate.bleu_score import corpus_bleu\n\ndef bleu():\n  true_imgs = list(test_data.keys())[:100]\n  actual = []\n  for refs in list(test_data.values())[:100]:\n    actual.append([x.split() for x in refs])\n  preds = []\n  i = 0\n  for im in true_imgs:\n    preds.append(generate_caption(im).split())\n    print(i)\n    i+=1\n\n  print('BLEU-1: %f' % corpus_bleu(actual, preds, weights=(1.0, 0, 0, 0)))\n  print('BLEU-2: %f' % corpus_bleu(actual, preds, weights=(0.5, 0.5, 0, 0)))\n  print('BLEU-3: %f' % corpus_bleu(actual, preds, weights=(0.3, 0.3, 0.3, 0)))\n  print('BLEU-4: %f' % corpus_bleu(actual, preds, weights=(0.25, 0.25, 0.25, 0.25)))\nbleu()  ","metadata":{"execution":{"iopub.status.busy":"2022-01-12T19:27:31.966965Z","iopub.execute_input":"2022-01-12T19:27:31.967843Z","iopub.status.idle":"2022-01-12T19:28:39.536529Z","shell.execute_reply.started":"2022-01-12T19:27:31.967797Z","shell.execute_reply":"2022-01-12T19:28:39.535767Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"def test_cap(img):\n    sample_img = decode_and_resize(img)\n    img = sample_img.numpy().clip(0, 255).astype(np.uint8)\n    plt.imshow(img)\n    plt.show()\n\n    # Pass the image to the CNN\n    img = tf.expand_dims(sample_img, 0)\n    img = caption_model.cnn_model(img)\n\n    # Pass the image features to the Transformer encoder\n    encoded_img = caption_model.encoder(img, training=False)\n\n    # Generate the caption using the Transformer decoder\n    decoded_caption = \"<start> \"\n    for i in range(max_decoded_sentence_length):\n        tokenized_caption = vectorization([decoded_caption])[:, :-1]\n        mask = tf.math.not_equal(tokenized_caption, 0)\n        predictions = caption_model.decoder(\n            tokenized_caption, encoded_img, training=False, mask=mask\n        )\n        sampled_token_index = np.argmax(predictions[0, i, :])\n        sampled_token = index_lookup[sampled_token_index]\n        if sampled_token == \" <end>\":\n            break\n        decoded_caption += \" \" + sampled_token\n\n    decoded_caption = decoded_caption.replace(\"<start> \", \"\")\n    decoded_caption = decoded_caption.replace(\" <end>\", \"\").strip()\n    print(\"Predicted Caption: \", decoded_caption)","metadata":{"execution":{"iopub.status.busy":"2022-01-12T19:29:28.564027Z","iopub.execute_input":"2022-01-12T19:29:28.564290Z","iopub.status.idle":"2022-01-12T19:29:28.573893Z","shell.execute_reply.started":"2022-01-12T19:29:28.564259Z","shell.execute_reply":"2022-01-12T19:29:28.573128Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"test_cap('../input/test-images/test images/landscape.jpg')","metadata":{"execution":{"iopub.status.busy":"2022-01-12T19:38:35.369061Z","iopub.execute_input":"2022-01-12T19:38:35.369667Z","iopub.status.idle":"2022-01-12T19:38:36.383807Z","shell.execute_reply.started":"2022-01-12T19:38:35.369621Z","shell.execute_reply":"2022-01-12T19:38:36.382909Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}